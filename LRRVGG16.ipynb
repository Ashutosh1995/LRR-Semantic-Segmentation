{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ## Laplacian Pyramid Reconstruction and Refinement for Semantic Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LRR_lib import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LRR32s()\n",
    "vgg16 = models.vgg16(pretrained=True)\n",
    "model.init_vgg16_params(vgg16)\n",
    "model = torch.nn.DataParallel(model, device_ids=range(torch.cuda.device_count()))\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variable passed to model\n",
    "Epoch = 100\n",
    "img_row = 224\n",
    "img_col = 224\n",
    "batch_size_voc = 6\n",
    "Momemtum = 0.9\n",
    "Weight_Decay = 5e-4\n",
    "Learning_Rate = 2e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_aug = Compose([RandomRotate(10),RandomHorizontallyFlip()])\n",
    "results = []\n",
    "d_scale1 = 4;\n",
    "data_loader = get_loader('pascal')\n",
    "data_path = get_data_path('pascal')\n",
    "\n",
    "t_loader = data_loader(data_path, is_transform=True, img_size=(img_row, img_col), augmentations=data_aug)\n",
    "v_loader = data_loader(data_path, is_transform=True, split='val', img_size=(img_row, img_col))\n",
    "\n",
    "n_classes = t_loader.n_classes\n",
    "trainloader = data.DataLoader(t_loader, batch_size=batch_size_voc, num_workers=8, shuffle=True)\n",
    "valloader = data.DataLoader(v_loader, batch_size=batch_size_voc, num_workers=8)\n",
    "\n",
    "running_metrics = runningScore(n_classes)\n",
    "    \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=Learning_Rate, momentum=Momemtum, weight_decay=Weight_Decay)\n",
    "\n",
    "loss_fn = cross_entropy2d\n",
    "\n",
    "best_iou = -100.0 \n",
    "for epoch in range(Epoch):\n",
    "    model.train()\n",
    "    for i, (images, labels) in enumerate(trainloader):\n",
    "        labels = label_downscale(labels, d_scale1, batch_size_voc, img_row);\n",
    "        images = Variable(images.cuda())\n",
    "        labels = Variable(labels.cuda()) \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)    \n",
    "        loss = loss_fn(input=outputs, target=labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (i+1)*batch_size_voc > 8600:\n",
    "           break;\n",
    "        if (i+1) % 10 == 0:\n",
    "            print(\"Epoch [%d/%d/%d] Loss: %.10f\" % ((i+1)*batch_size_voc,epoch+1, Epoch, loss.data[0]))\n",
    "    model.eval()\n",
    "            \n",
    "    for i_val, (images_val, labels_val) in tqdm(enumerate(valloader)):\n",
    "        if labels_val.size()[0]== batch_size_voc :\n",
    "                labels_val = label_downscale(labels_val, d_scale1, batch_size_voc, img_row);\n",
    "                images_val = Variable(images_val.cuda(), volatile=True)\n",
    "                labels_val = Variable(labels_val.cuda(), volatile=True)\n",
    "\n",
    "                outputs = model(images_val)\n",
    "                \n",
    "                pred = outputs.data.max(1)[1].cpu().numpy()\n",
    "                gt = labels_val.data.cpu().numpy()\n",
    "                running_metrics.update(gt, pred)\n",
    "\n",
    "    score, class_iou = running_metrics.get_scores()\n",
    "    for k, v in score.items():\n",
    "        print(k, v)\n",
    "        results.append(v)\n",
    "        running_metrics.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LRR()\n",
    "model.init_decov_2x_4x_params()\n",
    "model = torch.nn.DataParallel(model, device_ids=range(torch.cuda.device_count()))\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variable passed to model\n",
    "Epoch = 150\n",
    "img_row = 224\n",
    "img_col = 224\n",
    "batch_size_voc = 6\n",
    "Momemtum = 0.9\n",
    "Weight_Decay = 5e-4\n",
    "Learning_Rateft = 1e-5\n",
    "Learning_Rate4x = 1e-4\n",
    "Learning_Rate2x = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_aug = Compose([RandomRotate(10),RandomHorizontallyFlip()])\n",
    "results = []\n",
    "scale8x = 4;\n",
    "scale4x = 2;\n",
    "\n",
    "data_loader = get_loader('pascal')\n",
    "data_path = get_data_path('pascal')\n",
    "print(data_path)\n",
    "\n",
    "t_loader = data_loader(data_path, is_transform=True, img_size=(img_row, img_col), augmentations=data_aug)\n",
    "v_loader = data_loader(data_path, is_transform=True, split='val', img_size=(img_row, img_col))\n",
    "\n",
    "n_classes = t_loader.n_classes\n",
    "trainloader = data.DataLoader(t_loader, batch_size=batch_size_voc, num_workers=8, shuffle=True)\n",
    "valloader = data.DataLoader(v_loader, batch_size=batch_size_voc, num_workers=8)\n",
    "\n",
    "# Setup Metrics\n",
    "running_metrics = runningScore(n_classes)\n",
    "\n",
    "optimizerft = torch.optim.SGD(model.parameters(), lr=Learning_Rateft, momentum=Momemtum)\n",
    "optimizer4x = torch.optim.SGD(model.parameters(), lr=Learning_Rate4x, momentum=Momemtum,weight_decay=Weight_Decay)\n",
    "optimizer2x = torch.optim.SGD(model.parameters(), lr=Learning_Rate2x, momentum=Momemtum,weight_decay=Weight_Decay)\n",
    "\n",
    "loss_fn = cross_entropy2d\n",
    "\n",
    "best_iou = -100.0 \n",
    "for epoch in range(Epoch):\n",
    "    model.train()\n",
    "    for i, (images, labels) in enumerate(trainloader):\n",
    "        \n",
    "        labels8x = label_downscale(labels, scale8x, batch_size_voc, img_row);\n",
    "        labels4x = label_downscale(labels, scale4x, batch_size_voc, img_row);\n",
    "        \n",
    "        images = Variable(images.cuda())\n",
    "        labels2x = Variable(labels.cuda())\n",
    "        labels4x = Variable(labels4x.cuda())\n",
    "        labels8x = Variable(labels8x.cuda())\n",
    "               \n",
    "        outputs8x,outputs4x,outputs2x = model(images)        \n",
    "        \n",
    "        if epoch >= 100 and epoch <=Epoch:    \n",
    "            optimizerft.zero_grad()        \n",
    "            loss8x = loss_fn(input=outputs8x, target=labels8x)\n",
    "            loss8x.backward(retain_graph=True)\n",
    "            optimizerft.step()\n",
    "            \n",
    "            optimizerft.zero_grad()        \n",
    "            loss4x = loss_fn(input=outputs4x, target=labels4x)\n",
    "            loss4x.backward(retain_graph=True)\n",
    "            optimizerft.step()\n",
    "            \n",
    "            optimizerft.zero_grad()        \n",
    "            loss2x = loss_fn(input=outputs2x, target=labels2x)\n",
    "            loss2x.backward()\n",
    "            optimizerft.step()\n",
    "            print(\"Epoch [%d/%d/%d] Loss4x: %.4f \" % ((i+1)*batch_size_voc,epoch+1, Epoch,loss2x.data[0]))\n",
    "        if epoch >= 0 and epoch <=50:        \n",
    "            optimizer4x.zero_grad()        \n",
    "            loss4x = loss_fn(input=outputs4x, target=labels4x)\n",
    "            loss4x.backward()\n",
    "            optimizer4x.step()\n",
    "            print(\"Epoch [%d/%d/%d] Loss4x: %.4f \" % ((i+1)*batch_size_voc,epoch+1, Epoch,loss4x.data[0]))\n",
    "            \n",
    "        if epoch >50  and epoch <=100:\n",
    "            optimizer2x.zero_grad()        \n",
    "            loss2x = loss_fn(input=outputs2x, target=labels2x)\n",
    "            loss2x.backward()\n",
    "            optimizer2x.step()\n",
    "            print(\"Epoch [%d/%d/%d] Loss4x: %.4f \" % ((i+1)*batch_size_voc,epoch+1, Epoch,loss2x.data[0]))    \n",
    "        \n",
    "        if (i+1)*batch_size_voc > 8700:\n",
    "           break;\n",
    "\n",
    "    model.eval()\n",
    "            \n",
    "    for i_val, (images_val, labels_val) in tqdm(enumerate(valloader)):\n",
    "        if labels_val.size()[0]== batch_size_voc :\n",
    "            \n",
    "                images_val = Variable(images_val.cuda(), volatile=True)\n",
    "                labels_val = Variable(labels_val.cuda(), volatile=True)\n",
    "\n",
    "                outputs8x,outputs4x,outputs2x = model(images_val)\n",
    "                \n",
    "                pred = outputs2x.data.max(1)[1].cpu().numpy()\n",
    "                gt = labels_val.data.cpu().numpy()\n",
    "                running_metrics.update(gt, pred)\n",
    "\n",
    "    score, class_iou = running_metrics.get_scores()\n",
    "    for k, v in score.items():\n",
    "        print(k, v)\n",
    "        results.append(v)\n",
    "        running_metrics.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
